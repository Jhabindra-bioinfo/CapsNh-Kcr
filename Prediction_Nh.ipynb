{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of kcr sites in non-histone proteins. The threshold is 0.5.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras import initializers,layers,regularizers\n",
    "from keras.layers import Dropout\n",
    "from keras import callbacks\n",
    "from keras.models import *\n",
    "from keras.engine.topology import Layer\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dropout,Activation\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from scipy import interp\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense,Input,Activation,Conv1D,BatchNormalization,Concatenate\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.utils import class_weight\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.4 (default, Aug 13 2019, 20:35:49) \\n[GCC 7.3.0]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To read the fasta file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_fasta(file_path):\n",
    "    one=list(SeqIO.parse(file_path,'fasta'))\n",
    "    return one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To encode the amino acid using popular one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot(seq):\n",
    "    bases = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n",
    "    X = np.zeros((len(seq),len(seq[0]),len(bases)))\n",
    "    for i,m in enumerate(seq):\n",
    "        for l,s in enumerate(m):\n",
    "    #         print(s)\n",
    "            if s in bases:\n",
    "                X[i,l,bases.index(s)] = 1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed capsnet model core code stares from here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squash Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  \n",
    "    The non-linear activation used in Capsule. It drives the length of a large vector to near 1 and small vector to 0\n",
    "    :param vectors: some vectors to be squashed, N-dim tensor\n",
    "    :param axis: the axis to squash\n",
    "    :return: a Tensor with same shape as input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(vectors, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Length(layers.Layer):\n",
    "    def call(self, inputs, **kwargs):\n",
    "        return K.sqrt(K.sum(K.square(inputs), -1))\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capsule Layer\n",
    "The capsule layer. It is similar to Dense layer. Dense layer has `in_num` inputs, each is a scalar, the output of the\n",
    "    neuron from the former layer, and it has `out_num` output neurons. CapsuleLayer just expand the output of the neuron\n",
    "    from scalar to vector. So its input shape = [None, input_num_capsule, input_dim_vector] and output shape = \\\n",
    "    [None, num_capsule, dim_vector]. For Dense Layer, input_dim_vector = dim_vector = 1.\n",
    "    :param num_capsule: number of capsules in this layer\n",
    "    :param dim_vector: dimension of the output vectors of the capsules in this layer\n",
    "    :param num_routings: number of iterations for the routing algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsule, dim_vector, num_routing=3,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_vector = dim_vector\n",
    "        self.num_routing = num_routing\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 3, \"The input Tensor should have shape=[None, input_num_capsule, input_dim_vector]\"\n",
    "        self.input_num_capsule = input_shape[1]\n",
    "        self.input_dim_vector = input_shape[2]\n",
    "\n",
    "        # Transform matrix\n",
    "        self.W = self.add_weight(\n",
    "            shape=[self.input_num_capsule, self.num_capsule, self.input_dim_vector, self.dim_vector],\n",
    "            initializer=self.kernel_initializer,\n",
    "            name='W')\n",
    "\n",
    "        # Coupling coefficient. The redundant dimensions are just to facilitate subsequent matrix calculation.\n",
    "        self.bias = self.add_weight(shape=[1, self.input_num_capsule, self.num_capsule, 1, 1],\n",
    "                                    initializer=self.bias_initializer,\n",
    "                                    name='bias',\n",
    "                                    trainable=False)\n",
    "        self.built = True\n",
    "    def call(self, inputs, training=None):\n",
    "        # inputs.shape=[None, input_num_capsule, input_dim_vector]\n",
    "        # Expand dims to [None, input_num_capsule, 1, 1, input_dim_vector]\n",
    "        inputs_expand = K.expand_dims(K.expand_dims(inputs, 2), 2)\n",
    "\n",
    "        # Replicate num_capsule dimension to prepare being multiplied by W\n",
    "        # Now it has shape = [None, input_num_capsule, num_capsule, 1, input_dim_vector]\n",
    "        inputs_tiled = K.tile(inputs_expand, [1, 1, self.num_capsule, 1, 1])\n",
    "\n",
    "        \"\"\"\n",
    "        # Begin: inputs_hat computation V1 ---------------------------------------------------------------------#\n",
    "        # Compute `inputs * W` by expanding the first dim of W. More time-consuming and need batch_size.\n",
    "        # w_tiled.shape = [batch_size, input_num_capsule, num_capsule, input_dim_vector, dim_vector]\n",
    "        w_tiled = K.tile(K.expand_dims(self.W, 0), [self.batch_size, 1, 1, 1, 1])\n",
    "        # Transformed vectors, inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = K.batch_dot(inputs_tiled, w_tiled, [4, 3])\n",
    "        # End: inputs_hat computation V1 ---------------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "\n",
    "        # Begin: inputs_hat computation V2 ---------------------------------------------------------------------#\n",
    "        # Compute `inputs * W` by scanning inputs_tiled on dimension 0. This is faster but requires Tensorflow.\n",
    "        # inputs_hat.shape = [None, input_num_capsule, num_capsule, 1, dim_vector]\n",
    "        inputs_hat = tf.scan(lambda ac, x: K.batch_dot(x, self.W, [3, 2]),\n",
    "                             elems=inputs_tiled,\n",
    "                             initializer=K.zeros([self.input_num_capsule, self.num_capsule, 1, self.dim_vector]))\n",
    "        # End: inputs_hat computation V2 ---------------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "        # Begin: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        def body(i, b, outputs):\n",
    "            c = tf.nn.softmax(b, dim=2)  # dim=2 is the num_capsule dimension\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "            if i != 1:\n",
    "                b = b + K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "            return [i-1, b, outputs]\n",
    "        cond = lambda i, b, inputs_hat: i > 0\n",
    "        loop_vars = [K.constant(self.num_routing), self.bias, K.sum(inputs_hat, 1, keepdims=True)]\n",
    "        shape_invariants = [tf.TensorShape([]),\n",
    "                            tf.TensorShape([None, self.input_num_capsule, self.num_capsule, 1, 1]),\n",
    "                            tf.TensorShape([None, 1, self.num_capsule, 1, self.dim_vector])]\n",
    "        _, _, outputs = tf.while_loop(cond, body, loop_vars, shape_invariants)\n",
    "        # End: routing algorithm V1, dynamic ------------------------------------------------------------#\n",
    "        \"\"\"\n",
    "\n",
    "        # Begin: routing algorithm V2, static -----------------------------------------------------------#\n",
    "        # Routing algorithm V2. Use iteration. V2 and V1 both work without much difference on performance\n",
    "        assert self.num_routing > 0, 'The num_routing should be > 0.'\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(self.bias, dim=2)\n",
    "            \n",
    "            # dim=2 is the num_capsule dimension\n",
    "            # outputs.shape=[None, 1, num_capsule, 1, dim_vector]\n",
    "            outputs = squash(K.sum(c * inputs_hat, 1, keepdims=True))\n",
    "\n",
    "            # last iteration needs not compute bias which will not be passed to the graph any more anyway.\n",
    "            if i != self.num_routing - 1:\n",
    "                # self.bias = K.update_add(self.bias, K.sum(inputs_hat * outputs, [0, -1], keepdims=True))\n",
    "                self.bias += K.sum(inputs_hat * outputs, -1, keepdims=True)\n",
    "                # tf.summary.histogram('BigBee', self.bias)  # for debugging\n",
    "        # End: routing algorithm V2, static ------------------------------------------------------------#\n",
    "\n",
    "        return K.reshape(outputs, [-1, self.num_capsule, self.dim_vector])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.num_capsule, self.dim_vector])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PrimeryCap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PrimaryCap(inputs, dim_vector, n_channels, kernel_size, strides, padding,dropout):\n",
    "    output = layers.Conv1D(filters=dim_vector * n_channels, kernel_size=kernel_size, strides=strides, padding=padding, \n",
    "                           name='primarycap_conv1d')(inputs)\n",
    "    outputs = layers.Reshape(target_shape=[-1, dim_vector], name='primarycap_reshape')(output)\n",
    "    return layers.Lambda(squash, name='primarycap_squash')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CapsNet(input_shape, n_class, num_routing):\n",
    "    x = layers.Input(shape=input_shape)\n",
    "    conv1 = layers.Conv1D(filters=32, kernel_size=7, strides=1, padding='valid', activation='relu', name='conv1')(x)\n",
    "    conv1=Dropout(0.7)(conv1)\n",
    "    primarycaps = PrimaryCap(conv1, dim_vector=8, n_channels=16, kernel_size=7, strides=1, padding='valid',dropout=0.2)\n",
    "    KcrCaps = CapsuleLayer(num_capsule=n_class, dim_vector=8, num_routing=num_routing,kernel_initializer='he_normal', name='KcrCaps')(primarycaps)\n",
    "    out = Length(name='capsnet')(KcrCaps)\n",
    "    train_model = Model(x, out)\n",
    "    return train_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CapsNh-Kcr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 29, 20)            0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 23, 32)            4512      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 23, 32)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_conv1d (Conv1D)   (None, 17, 128)           28800     \n",
      "_________________________________________________________________\n",
      "primarycap_reshape (Reshape) (None, 272, 8)            0         \n",
      "_________________________________________________________________\n",
      "primarycap_squash (Lambda)   (None, 272, 8)            0         \n",
      "_________________________________________________________________\n",
      "KcrCaps (CapsuleLayer)       (None, 2, 8)              35360     \n",
      "_________________________________________________________________\n",
      "capsnet (Length)             (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 68,672\n",
      "Trainable params: 68,128\n",
      "Non-trainable params: 544\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mode1= CapsNet(input_shape=(29,20),n_class=2,num_routing=3)\n",
    "mode1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload a data for prediction without labeling the data.  In our case,  here, we used the independent data (as described in the manuscript)  for prediction.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data= read_fasta('Kcr_IND.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred():\n",
    "    positive_ind_data=positive_ind_data=all_data[0:3343] # positive data # use read_rasta function written in the beginning\n",
    "    print(\"The total length of positive sequences to be predicted is:\", len(positive_ind_data))\n",
    "    negative_ind_data=all_data[3344:6687] #negative data\n",
    "    print(\"The total length of negative sequences to be predicted is:\", len(negative_ind_data))\n",
    "    all_ind_data=positive_ind_data+negative_ind_data\n",
    "    all_ind_onehot=onehot(all_ind_data) # use onehot function  written in the beginning\n",
    "    all_pred_y=[]\n",
    "    for i in range(5):\n",
    "        mode1.load_weights(str(i+0)+'nonhistoneLastepoch_new'+'.h5')\n",
    "        pred_y=mode1.predict(all_ind_onehot)\n",
    "        #all_pred_y.append(pred_y)\n",
    "        pred_y=np.argmax(pred_y, axis=1)\n",
    "        all_pred_y.append(pred_y)\n",
    "        print(pred_y)\n",
    "    all_pred_y=np.average(all_pred_y,axis=0)\n",
    "    print(all_pred_y)\n",
    "    return all_pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total length of positive sequences to be predicted is: 3343\n",
      "The total length of negative sequences to be predicted is: 3343\n",
      "[1 1 1 ... 0 0 0]\n",
      "[1 1 1 ... 0 0 0]\n",
      "[1 1 1 ... 0 0 0]\n",
      "[1 1 1 ... 0 1 0]\n",
      "[1 1 1 ... 0 0 0]\n",
      "[1.  1.  1.  ... 0.  0.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "result=get_pred()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of first 500 sequence (positive sequence or kcr site containing sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result shows prediction for first 1000 positive sequence :  \n",
      " [1.  1.  1.  1.  1.  1.  1.  1.  1.  0.4 0.  1.  1.  0.  1.  1.  0.  1.\n",
      " 0.4 0.2 1.  1.  1.  0.6 1.  1.  0.6 0.  1.  0.8 1.  1.  0.  1.  1.  1.\n",
      " 1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0.6 1.  1.  1.  1.  1.  1.  0.2\n",
      " 1.  0.6 1.  1.  1.  0.  0.  0.  0.6 0.  0.2 1.  1.  1.  1.  0.4 1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  0.2 0.  1.  1.  1.  1.  1.  0.  1.  1.  1.\n",
      " 1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  0.8 1.  1.  1.  1.  0.6 1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  0.2 1.  0.  0.4 1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  0.  1.  1.  1.  0.  1.  0.8 0.4 1.  1.  0.2 0.8 1.\n",
      " 1.  1.  1.  1.  0.8 1.  1.  1.  1.  1.  0.2 0.2 1.  1.  1.  1.  1.  1.\n",
      " 1.  1.  0.  1.  1.  1.  1.  0.  1.  1.  1.  1.  0.8 1.  1.  0.6 1.  0.\n",
      " 1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0.2 0.  1.  1.  1.  1.\n",
      " 1.  0.8 1.  1.  1.  0.6 1.  0.  1.  1.  1.  1.  1.  1.  0.6 1.  1.  1.\n",
      " 0.  1.  1.  1.  1.  0.6 0.  1.  1.  1.  0.6 0.  1.  1.  1.  0.  0.  1.\n",
      " 1.  1.  1.  1.  1.  0.  1.  1.  0.8 1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  1.  0.8 1.  1.  1.  1.  0.2 0.8 0.2 1.  1.  1.  1.  0.  1.  1.  1.\n",
      " 1.  0.  1.  1.  0.  1.  1.  0.  1.  1.  1.  0.2 1.  1.  1.  1.  1.  1.\n",
      " 1.  1.  1.  0.4 1.  0.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  0.8\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  0.  1.\n",
      " 0.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.6 1.  1.  1.  1.  1.  0.6\n",
      " 0.  0.8 1.  0.4 1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  1.  0.  0.  0.6 1.  1.  1.  1.  1.  0.8 1.  1.  1.  1.  0.  1.  1.\n",
      " 1.  1.  0.  1.  0.  1.  1.  1.  1.  1.  1.  1.  0.6 1.  1.  0.  1.  1.\n",
      " 1.  1.  1.  0.  0.2 1.  1.  1.  1.  0.8 1.  0.  1.  1.  1.  0.4 1.  1.\n",
      " 1.  1.  1.  0.8 1.  0.  1.  1.  1.  1.  1.  0.2 1.  1.  1.  1.  1.  1.\n",
      " 1.  0.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  0.6 1.  0.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  0.6 1.  1.  1.  0.8 1.  1.  0.8 1.  1.  1.  1.  1.  0.  1.  1.  0.\n",
      " 0.8 1.  1.  1.  1.  0.8 1.  1.  0.  1.  0.2 1.  1.  1.  0.  1.  1.  1.\n",
      " 0.  0.  1.  1.  1.  1.  1.  0.8 1.  1.  1.  1.  1.  1.  1.  0.  1.  1.\n",
      " 1.  0.  1.  1.  1.  1.  1.  1.  0.8 1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  0.2 1.  1.  1.  0.  1.  0.  1.  1.  1.  0.  1.  0.  1.  1.  1.  1.\n",
      " 1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  0.6 1.  0.  1.  0.6 0.  1.  0.\n",
      " 1.  1.  0.2 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  0.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.  1.  1.  1.\n",
      " 1.  1.  0.8 0.  1.  1.  1.  0.6 0.8 1.  1.  1.  0.6 1.  1.  1.  0.  1.\n",
      " 1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.8 1.\n",
      " 1.  1.  1.  0.  1.  1.  1.  0.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  0.4 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.\n",
      " 1.  1.  0.2 1.  0.6 0.  1.  0.8 1.  1.  1.  1.  1.  0.4 1.  1.  1.  1.\n",
      " 1.  1.  1.  1.  0.  1.  0.8 1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  0.8 1.  0.8 1.  1.  0.6 1.  1.  1.  1.  0.6 1.  1.  1.  1.  0.2 1.\n",
      " 1.  1.  1.  1.  1.  0.  1.  0.4 1.  1.  0.8 1.  1.  1.  1.  0.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  0.  1.  1.\n",
      " 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.4 1.  1.  1.  1.\n",
      " 1.  1.  0.6 1.  1.  1.  1.  1.  1.  1.  0.  0.4 1.  0.8 1.  0.6 1.  1.\n",
      " 0.2 1.  1.  1.  0.8 1.  1.  0.  1.  1.  1.  1.  0.6 0.8 1.  0.6 1.  0.\n",
      " 1.  1.  1.  0.8 1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.  1.  1.  1.\n",
      " 1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.  1.  0.  1.\n",
      " 1.  1.  1.  0.2 0.8 1.  1.  1.  1.  1.  0.4 0.8 1.  1.  1.  1.  1.  0.\n",
      " 1.  1.  1.  1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.6\n",
      " 0.8 0.2 1.  1.  1.  1.  1.  1.  1.  1.  0.  0.6 0.8 1.  1.  1.  1.  1.\n",
      " 1.  1.  0.8 0.  0.2 1.  1.  0.  1.  1.  1.  0.8 0.  0.8 1.  0.  1.  0.\n",
      " 0.  1.  0.2 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n",
      " 1.  1.  0.  1.  1.  1.  1.  1.  1.  1.  1.  0.8 1.  1.  1.  1.  0.  1.\n",
      " 1.  1.  1.  0.  1.  0.  0.6 1.  0.  1. ]\n"
     ]
    }
   ],
   "source": [
    "print(\"The result shows prediction for first 500 positive sequence :\",' \\n', result[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction of last 500 negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result shows prediction for last 500 negative sequence :  \n",
      " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.4 0.  0.  0.  0.  0.\n",
      " 0.  0.  1.  0.  0.  0.  1.  0.  0.  0.4 0.  0.  0.  0.  0.  0.  1.  1.\n",
      " 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.2 1.  0.  1.  0.  0.  0.2 0.\n",
      " 0.  0.  0.2 0.2 0.  1.  0.  1.  1.  0.  1.  0.  0.  0.  1.  0.  0.  0.\n",
      " 0.2 0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.\n",
      " 0.  1.  0.  1.  0.  1.  1.  0.6 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.2\n",
      " 0.8 0.  0.  0.6 0.  1.  1.  0.  0.8 0.  1.  0.  1.  0.  1.  0.4 1.  0.\n",
      " 0.  0.  1.  0.  0.  1.  0.  0.  1.  0.  0.8 0.  1.  0.  1.  0.  0.4 0.\n",
      " 0.  0.  0.  1.  0.  0.  0.  1.  0.  1.  0.  1.  0.  0.  0.2 0.  0.  0.\n",
      " 0.  0.  0.  0.  1.  1.  1.  0.8 0.  0.  0.  0.  0.  0.  1.  1.  1.  0.\n",
      " 1.  0.8 0.6 0.  0.  0.2 0.4 0.2 1.  0.  1.  0.  0.  0.  0.  0.8 1.  0.\n",
      " 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  1.\n",
      " 1.  0.  1.  0.  0.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.6 0.  0.\n",
      " 0.4 0.  0.  0.4 0.  0.8 0.  0.  1.  0.  0.  1.  0.  0.  0.  0.4 0.  0.\n",
      " 0.2 1.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  1.  0.  0.\n",
      " 1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.8 0.  0.  0.  0.4 0.  0.  0.\n",
      " 0.  0.  0.  0.  1.  0.6 0.  0.  0.  0.  0.4 0.  0.  0.  0.  0.  0.2 1.\n",
      " 0.  0.  0.  1.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  1.  0.2\n",
      " 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.4 0.  1.  0.6\n",
      " 0.6 0.  1.  0.8 0.6 0.  0.  1.  1.  0.  0.8 0.  0.  0.4 0.  0.  1.  0.\n",
      " 1.  1.  0.  0.  1.  0.6 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
      " 0.  0.  0.  0.8 0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.2 0.\n",
      " 0.  0.6 1.  0.  1.  1.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      " 0.  1.  0.  0.8 1.  0.2 0.  1.  0.  0.  0.  0.  0.  0.  1.  1.  0.6 0.\n",
      " 0.  0.  1.  1.  0.  0.  0.6 0.2 1.  0.  1.  0.  1.  0.  0.  0.  0.  0.\n",
      " 1.  0.4 0.  0.  0.  0.  0.  1.  0.  0.  1.  1.  0.  0.  0.  1.  0.  1.\n",
      " 1.  0.  1.  1.  0.  0.  0.  0.6 1.  0.  1.  0.  1.  0.  0.  0.  1.  0.2\n",
      " 0.  0.  0.6 0.  0.  1.  0.4 0.  0.  0.  0.  0.  0.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "print(\"The result shows prediction for last 500 negative sequence :\",' \\n', result[-500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
